---
title: "ww_arg_1"
author: "Sooyeol Kim"
date: "`r Sys.Date()`"
output: html_document
---

The following code was used to produce the final ARG concentration dataset. 

# 1. Data setup 
Importing packages and data followed with clean-up of data according to QA/QC parameters. 

## 1.1 Packages needed for data analysis and plotting 
```{r r_setup, include=FALSE}
#Packages 
library(readr) #package for reading datasheets
library(dplyr) #package for data manipulation (mutate, filter, etc)
library(ggplot2) #package for plotting 
library(tidyr) #package for tidy data 
library(stringr) #package for working with strings 
library(pheatmap) #package for making heatmaps
library(maps) #package to plot on maps
library(ggmap)
library(zipcodeR)
library(mapproj)
library(EnvStats) #package for stats test like Rosner's
library(viridisLite) #package for viridis color palette
library(sf) #simple features package useful for handling spatial data
```

## 1.2 Import raw data and clean 

Downloading dPCR results of the ARGs and 16S/rpoB, currently in csv files in the same folder as this R Markdown. Each csv used without renaming and has already been matched for sample location/date/solid content to calculate the resulting concentration on the spreadsheet for those that resulted in 3 or more positive droplets (detect). 

On the EVA sheet, empty wells have been taken out. Plate 4 rpoB plate had been mislabeled and was corrected (plate same as plate map except E11 -> A11). Plate 2 rpoB well H10 also mislabeled so corrected. 

On the metadata sheet, 80-240522 was present twice (same exact information) so one row was removed. Site name for 129-240521 was corrupted so it was corrected. 

```{r data_import}
#ARG results 
data_01_ARG_raw <- read_csv("dPCR_ARG_Compiled - AllData_Final.csv")

#EvaGreen results 
data_01_EVA_raw <- read_csv("dPCR_EVA_Compiled - Sample.csv")

#EvaGreen controls
data_01_ctr_raw <- read_csv("dPCR_EVA_Compiled - Control.csv")

#Metadata
data_01_met_raw <- read_csv("ARG_project_metadata.csv")

#Selecting needed columns from each dataset to keep the data frame more manageable and renaming for ease of coding 
data_02_ARG_mod <- select(data_01_ARG_raw, `Sample description 1`, `Accepted Droplets`, Positives, MergedWells, `Conc(copies/µL)...8`, plant, date, percent_solids, dry_mass, target, dilution, dry_gc_g, uci_68_dry, lci_68_dry, gc_ul_template, uci_68_gc_ul_template, lci_68_gc_ul_template) %>%
  rename(sample_id = `Sample description 1`,
         accepted = `Accepted Droplets`, 
         positives = Positives, 
         merged_wells = MergedWells,
         conc_ul = `Conc(copies/µL)...8`)

data_02_EVA_mod <- select(data_01_EVA_raw, Plate, `Sample description 1`, `Sample description 2`, Target, `Conc(copies/µL)`, `Accepted Droplets`, Positives, plant, date, percent_solids, dry_mass, dry_gc_g, gc_ul_template) %>% 
  rename(plate = Plate,
         sample_id = `Sample description 1`, 
         dilution = `Sample description 2`, 
         target = Target, 
         conc_ul = `Conc(copies/µL)`,
         accepted = `Accepted Droplets`, 
         positives = Positives)

data_02_ctr_mod <- select(data_01_ctr_raw, Plate, `Sample description 1`, `Sample description 2`, Target, `Conc(copies/µL)`, `Accepted Droplets`, Positives) %>% 
  rename(plate = Plate, 
         sample_id = `Sample description 1`, 
         dilution = `Sample description 2`, 
         target = Target, 
         conc_ul = `Conc(copies/µL)`,
         accepted = `Accepted Droplets`, 
         positives = Positives)

data_02_met_mod <- select(data_01_met_raw, Sample_ID, City, State, Plant, Site_Name, Zipcode, Population_Served, County_FIPS, PMMoV_gc_g_dry_weight) %>% 
  rename(sample_id = Sample_ID,
         city = City, 
         state = State, 
         plant = Plant, 
         site = Site_Name, 
         zipcode = Zipcode, 
         population = Population_Served, 
         fips = County_FIPS,
         PMMoV_gc_g = PMMoV_gc_g_dry_weight)

#Taking out periods from site names to be matched with the secondary datasets 
data_02_met_mod$site <- gsub("\\.", "", data_02_met_mod$site)
  
```

Check class to make sure everything has been imported in the right format and correct if currently not in the right format.

```{r data_class}
#Checking class 
as.data.frame(sapply(data_02_ARG_mod, class))
as.data.frame(sapply(data_02_EVA_mod, class))
as.data.frame(sapply(data_02_ctr_mod, class))
as.data.frame(sapply(data_02_met_mod, class))

#Correct dates and numerics that are originally in character form 
data_02_ARG_mod$date <- as.Date(data_02_ARG_mod$date, "%m/%d/%Y")
data_02_ARG_mod$dry_gc_g <- as.numeric(data_02_ARG_mod$dry_gc_g)
data_02_ARG_mod$uci_68_dry <- as.numeric(data_02_ARG_mod$uci_68_dry)
data_02_ARG_mod$lci_68_dry <- as.numeric(data_02_ARG_mod$lci_68_dry)
data_02_ARG_mod$gc_ul_template <- as.numeric(data_02_ARG_mod$gc_ul_template)
data_02_ARG_mod$uci_68_gc_ul_template <- as.numeric(data_02_ARG_mod$uci_68_gc_ul_template)
data_02_ARG_mod$lci_68_gc_ul_template <- as.numeric(data_02_ARG_mod$lci_68_gc_ul_template)

data_02_EVA_mod$date <- as.Date(data_02_EVA_mod$date, "%m/%d/%Y")
data_02_EVA_mod$conc_ul <- as.numeric(data_02_EVA_mod$conc_ul)
data_02_EVA_mod$dry_gc_g <- as.numeric(data_02_EVA_mod$dry_gc_g)
data_02_EVA_mod$gc_ul_template <- as.numeric(data_02_EVA_mod$gc_ul_template)

data_02_met_mod$zipcode <- as.character(data_02_met_mod$zipcode)
#Correct zipcodes that are missing the initial zero 
for (i in 1:length(data_02_met_mod$sample_id)) {
  if (!is.na(data_02_met_mod$zipcode[i])) {
    if (nchar(data_02_met_mod$zipcode[i]) < 5) {
      data_02_met_mod$zipcode[i] <- paste("0", data_02_met_mod$zipcode[i], sep = '')
    }
  }
}
```

Samples that had less than 3 positive droplets already have been set as NA for calculated results (like dry weight cp/g) but will also set the dPCR concentration to 0 if less than 3 droplets positive. 

```{r data_min}
#Setting minimum quantifiable number of positive droplets to 3 for all ARG data 
data_temp_below <- subset(data_02_ARG_mod, positives < 3)
data_temp_above <- subset(data_02_ARG_mod, positives >= 3)

data_temp_below$conc_ul <- 0

data_03_ARG_mod <- rbind(data_temp_below, data_temp_above)

#Setting minimum quantifiable number of positive droplets to 3 for all EVA data
data_temp_below <- subset(data_02_EVA_mod, positives < 3)
data_temp_above <- subset(data_02_EVA_mod, positives >= 3)

data_temp_below$conc_ul <- 0

data_03_EVA_mod <- rbind(data_temp_below, data_temp_above)

#Setting minimum quantifiable number of positive droplets to 3 for all EVA control data
data_temp_below <- subset(data_02_ctr_mod, positives < 3)
data_temp_above <- subset(data_02_ctr_mod, positives >= 3)

data_temp_below$conc_ul <- 0

data_03_ctr_mod <- rbind(data_temp_below, data_temp_above)
```

## 1.3 QA/QC 
1. Number of droplets at least over 10k 
2. All NTC non-detect or at least less than 1/10 of sample concentration: This has been checked manually for ARG plates. All NTC was ND so needs to only be checked for EVA. 
3. If undetected, not from a diluted sample

```{r data_qaqc1}
#Original dimension of dataframes 
dim(data_03_ARG_mod)
dim(data_03_EVA_mod)
dim(data_03_ctr_mod)

#Data QC condition 1
data_04_ARG_pass <- filter(data_03_ARG_mod, accepted > 10000)
data_04_EVA_pass <- filter(data_03_EVA_mod, accepted > 10000)
data_04_ctr_pass <- filter(data_03_ctr_mod, accepted > 10000)

dim(data_04_ARG_pass)
dim(data_04_EVA_pass)
dim(data_04_ctr_pass)

#To know which samples were taken out 
filter(data_03_ARG_mod, accepted <= 10000)
filter(data_03_EVA_mod, accepted <= 10000)
```

Updated Aug 12, 2024: 
Original dimension of data_03_ARG_mod is 5550 x 17, of data_03_EVA_mod is 902 x 13, and of data_03_ctr_mod is 16 x 7. 
After QC1, data_04_ARG_pass is 5526 x 17 (24 measurements taken out), data_04_EVA_pass is 897 x 13 (5 measurements taken out), and data_04_ctr_pass is 16 x 8 (no measurements taken out). 

The 24 measurements taken out in ARG were: 
126-240516 low conc targets (redone)
147-240521 low conc targets (redone)
43-240521 low conc targets (redone)
89-240519 low conc targets (redone)

The five measurements taken out in EVA were: 
78-240519 16S (missed this one while looking at samples to redo)
70-240522 16S (redone)
205-240522 16S (redone)
183-240517 16S (redone)
121-240520 16S (redone)

QAQC #2 only to EVA assays since the ARG assays were checked manually already that all NTC were non detects. 

```{r data_qaqc2}
#Filter for any NTC that has detectable level of target 
NTC_flag <- filter(data_04_ctr_pass, conc_ul > 0)

#Examine plates to see the range of targets detected and compare to positive partitions in NTC 
#Empty data frame to put all the flagged data 
data_flag <- data.frame(matrix(ncol = 13, nrow = 0))
colnames(data_flag) <- colnames(data_04_EVA_pass)

#Put all data on plates that had detectable level of target in NTC in dataframe data_flag 
for (i in 1: length(NTC_flag$sample_id)) {
  flag_plate <- NTC_flag$plate[i]
  flag_target <- NTC_flag$target[i]
  
  data_flag <- rbind(data_flag, subset(data_04_EVA_pass, plate == flag_plate & target == flag_target))
}

#Compare affected data concentration with NTC contamination concentration 
flag_NTC_mer <- merge(NTC_flag, data_flag, by = c("plate", "target"), suffixes = c("_N", "_d")) %>% 
  mutate(flag_ratio = conc_ul_d/conc_ul_N)

#Samples that resulted in less than magnitude of a difference from NTC contamination 
flag_NTC_mer <- filter(flag_NTC_mer, flag_ratio < 10)

flag_NTC_mer
```

16S and rpoB for 78-240520, 16S for 176-240523, and 16S for 152-240521 was not at least a magnitude higher than the contamination concentration and therefore will be excluded from analysis. 

```{r data_qaqc3}
#Note whether a target was detected or not
data_04_ARG_pass <- mutate(data_04_ARG_pass, detect = NA)
data_04_EVA_pass <- mutate(data_04_EVA_pass, detect = NA)

for (i in 1:length(data_04_ARG_pass$sample_id)) {
  if (data_04_ARG_pass$conc_ul[i] > 0) {
    data_04_ARG_pass$detect[i] <- "D"
  } else {data_04_ARG_pass$detect[i] <- "ND"}
}

for (i in 1:length(data_04_EVA_pass$sample_id)) {
  if (data_04_EVA_pass$conc_ul[i] > 0) {
    data_04_EVA_pass$detect[i] <- "D"
  } else {data_04_EVA_pass$detect[i] <- "ND"}
}

#Look at data that was not detected but was also diluted for analysis 
flag_ND_ARG <- filter(data_04_ARG_pass, detect == "ND" & dilution != 1)
flag_ND_EVA <- filter(data_04_EVA_pass, detect == "ND" & dilution != 1)
```

In the ARG dataframe, 35 measurements were non-detects when analyzed diluted. 
Following measurements were redone and we have data for: 107-240520 OXA-48, 107-240522 OXA-48, 107-240523 OXA-48, 126-240519 OXA-48, 150-240517 OXA-48, 150-240520 OXA-48, 150-240522 OXA-48, 154-240520 OXA-48, 154-240522 OXA-48, 167-240517 OXA-48, 167-240522 OXA-48, 176-240523 CTX-M, OXA-48; 195-240517 OXA-48, 195-240520 OXA-48, 195-240522 OXA-48, 196-240517 OXA-48, 196-240520 OXA-48, 204-240523 OXA-48, 254-240517 CTX-M, 254-240522 OXA-48, 31-240516 OXA-48, 65-240517 CTX-M, OXA-48; 65-240520 OXA-48

Following measurements were either not redone or was still too diluted: 107-240520 MCR-1, MecA (Samples from 107 prediluted x10 so no undiluted sample); 154-240517 OXA-48 (redone with 1:10 but still not detected); 254-240517 OXA-48 (redone with 1:10 but still not detected); 254-240520 OXA-48 (redone with 1:10 but still not detected); 254-240517 OXA-48 (redone with 1:10 but still not detected); 154-240517 OXA-48 (redone with 1:10 but still not detected); 254-240520 OXA-48 (redone with 1:10 but still not detected)

No measurements in the EVA dataframe were non-detects. 

The 35 measurements flagged here will also be excluded for analysis. 

```{r data_exclude}
data_05_ARG_pass <- data_04_ARG_pass
data_05_EVA_pass <- data_04_EVA_pass

#Data points being excluded as a result of QAQC#2 
for (i in 1:length(flag_NTC_mer$sample_id_d)) {
  flag_sample = flag_NTC_mer$sample_id_d[i]
  flag_target = flag_NTC_mer$target[i]
  flag_plate = flag_NTC_mer$plate[i]
  data_05_EVA_pass <- subset(data_05_EVA_pass, target != flag_target | sample_id != flag_sample, plate != flag_plate)
}

#Data points being excluded as as result of QAQC#3
for (i in 1:length(flag_ND_ARG$sample_id)) {
  flag_sample = flag_ND_ARG$sample_id[i]
  flag_target = flag_ND_ARG$target[i]
  data_05_ARG_pass <- subset(data_05_ARG_pass, target != flag_target | sample_id != flag_sample | detect != "ND")
}

dim(data_05_ARG_pass)
dim(data_05_EVA_pass)
```

After QAQC, the dimensions for data_05_ARG_pass is 5491 x 18 (35 samples from #3 excluded) and for data_05_EVA_pass is 893 x 14 (4 samples from #2 excluded). 

## 1.4 Data selection & Calculation
There were some samples that were run twice with different dilutions so we have to select the final concentration for those samples. 

```{r data_select}
#Identifying ARG analytes run twice 
unique_id <- unique(data_05_ARG_pass$sample_id)
unique_target <- unique(data_05_ARG_pass$target)

#Calculating confidence interval range for ARG measurements 
data_05_ARG_pass <- mutate(data_05_ARG_pass, ci_68_dry = uci_68_dry - lci_68_dry)

#Data frame for duplicate measurements to look at effects of freeze-thaw 
data_dup_ARG <- data.frame(matrix(ncol = 19, nrow = 0))
colnames(data_dup_ARG) <- colnames(data_05_ARG_pass)

#Data frame to select one measurement to move forward in other analysis
data_06_ARG_filt <- data.frame(matrix(ncol = 19, nrow = 0))
colnames(data_06_ARG_filt) <- colnames(data_05_ARG_pass)

#Choosing the measurement with smaller confidence interval
for (i in 1:length(unique_id)) {
  i_sample <- unique_id[i]
  for (j in 1:length(unique_target)) {
    j_target <- unique_target[j]
    data_temp <- subset(data_05_ARG_pass, sample_id == i_sample & target == j_target)
    if (nrow(data_temp) > 1) {
      data_dup_ARG <- rbind(data_dup_ARG, data_temp)
      i_min = which(data_temp$ci_68_dry == min(data_temp$ci_68_dry))
      #If there's multiple minimum because the confidence interval range is the same 
      if (length(i_min) > 1) {
        data_06_ARG_filt <- rbind(data_06_ARG_filt, data_temp[i_min[1], ])
      } else {data_06_ARG_filt <- rbind(data_06_ARG_filt, data_temp[i_min, ]) }
    }else {data_06_ARG_filt <- rbind(data_06_ARG_filt, data_temp)}
  }
}

dim(data_06_ARG_filt)

#Identifying samples run twice in EVA 
unique_id <- unique(data_05_EVA_pass$sample_id)
unique_target <- unique(data_05_EVA_pass$target)

#Calculating confidence interval range 
#data_05_EVA_pass <- mutate(data_05_EVA_pass, ci_68_dry = uci_68_dry - lci_68_dry)

#Data frame for duplicate measurements to look at effects of freeze-thaw 
data_dup_EVA <- data.frame(matrix(ncol = 14, nrow = 0))
colnames(data_dup_EVA) <- colnames(data_05_EVA_pass)

#Data frame to select one measurement to move forward in other analysis
data_06_EVA_filt <- data.frame(matrix(ncol = 14, nrow = 0))
colnames(data_06_EVA_filt) <- colnames(data_05_EVA_pass)

#Selecting the bigger concentration for now since that happens to be the reruns 
for (i in 1:length(unique_id)) {
  i_sample <- unique_id[i]
  for (j in 1:length(unique_target)) {
    j_target <- unique_target[j]
    data_temp <- subset(data_05_EVA_pass, sample_id == i_sample & target == j_target)
    if (nrow(data_temp) > 1) {
      data_dup_EVA <- rbind(data_dup_EVA, data_temp)
      i_max = which(data_temp$dry_gc_g == max(data_temp$dry_gc_g))
      #If there's multiple minimum because the confidence interval range is the same 
      if (length(i_min) > 1) {
        data_06_EVA_filt <- rbind(data_06_EVA_filt, data_temp[i_min[1], ])
      } else {data_06_EVA_filt <- rbind(data_06_EVA_filt, data_temp[i_min, ]) }
    }else {data_06_EVA_filt <- rbind(data_06_EVA_filt, data_temp)}
  }
}

dim(data_06_EVA_filt)

```

There were 122 measurements that were rerun in the ARG data frame (total of 244 measurements). Choosing one of the measurements based on the confidence interval range, the dimension of the data frame is now 5369 x 19. For EVA assays, there were 8 measurements that were rerun (total of 16 measurements). Choosing one of the measurements based on the resulting concentration, the dimension of the data frame is 885 x 14. 

Now that we've selected our final set of data, we'll also calculate the lower measurement limit for each sample and substitute any non-detects with half of the lower measurement limit. Not done for the EVA data frame since no measurements were non-detects. 

```{r data_calculate}
#Calculating lower measurement limit
data_07_ARG_calc <- data_06_ARG_filt %>% 
  mutate(lml_ul = -log((accepted-3)/accepted)/0.000795,
         lml_template_ul = lml_ul*(20*(str_count(merged_wells, ",")+1))/(5*(str_count(merged_wells, ",")+1)),
         lml_dry_gc_g = lml_template_ul*(60*(str_count(merged_wells, ",")+1))/dry_mass*(0.795/0.85))
#volume empirically obtained from data as 0.000795 but also confirmed with BioRad that it should be 0.795nL for older versions of QX Manager

#For those that were ND, substitute with half of lml 
for (i in 1:length(data_07_ARG_calc$sample_id)) {
  if (data_07_ARG_calc$detect[i] == "ND") {
    data_07_ARG_calc$dry_gc_g[i] <- data_07_ARG_calc$lml_dry_gc_g[i]/2
  }
}

```

## 1.5 Primary data visualization check 
Let's look at the primary data that we have currently before linking them up with any secondary data sources to make sure everything looks ok. 

Calculating summary stats about ARG data as a whole. 
```{r ARG_summary}
#Merge data with metadata
data_08_ARG_mer <- merge(data_07_ARG_calc, data_02_met_mod, by = c("sample_id"), all = TRUE) %>% filter(!is.na(site))

#Summarize data 
data_08_ARG_sum <- data_08_ARG_mer %>% 
  group_by(site, plant.y, target) %>% 
  summarise(mean = mean(dry_gc_g, na.rm = TRUE),
            sd = sd(dry_gc_g, na.rm = TRUE), 
            n = n()) %>% 
  mutate(CoV = sd/mean)

#Number of unique samples 
dim(data_08_ARG_mer)
length(unique(data_08_ARG_mer$sample_id))
length(unique(data_08_ARG_mer$plant.y))
length(unique(data_08_ARG_mer$state))
length(unique(data_08_ARG_mer$city))

#Number of samples from each plant 
table(subset(data_08_ARG_sum, target == "TEM")$n)

#Samples that got filtered out due to lack of metadata
setdiff(unique(data_07_ARG_calc$sample_id), unique(data_08_ARG_mer$sample_id))

```

After QA/QC, we have 446 samples and 5345 measurements that represent 164 plants in 40 states and 134 cities in the US. This is excluding the 2 samples from site 87 that Dora said should be excluded because they don't get exported (87-240519, 87-240521). 

Plotting the distribution of ARGs for each target: 

```{r ARG_dist}
ggplot(data_08_ARG_mer, aes(x = target, y = dry_gc_g, color = target)) + 
  geom_jitter(aes(shape = detect), color = "darkgrey", size = 0.5, alpha = 0.9) +
  geom_boxplot(alpha = 0.7) +
  scale_y_continuous(trans = 'log10') + 
  scale_color_manual(values = c("#05386d","#54278f", "#08519c", "#fc9272", "#756bb1", "#3182bd", "#6baed6", "#9e9ac8", "#525252","#31a354","#a1d99b","#9ecaef" )) + 
  scale_shape_manual(values = c(19, 1)) +
  ylab("Conc (cp/g dry weight)") + 
  theme_bw() + 
  labs(title = "ARG distribution across all data")

ggplot(data_08_ARG_mer, aes(x = target, y = dry_gc_g/PMMoV_gc_g, color = target)) + 
  geom_jitter(aes(shape = detect), color = "darkgrey", size = 0.5, alpha = 0.9) +
  geom_boxplot(alpha = 0.7) +
  scale_y_continuous(trans = 'log10') + 
  scale_color_manual(values = c("#05386d","#54278f", "#08519c", "#fc9272", "#756bb1", "#3182bd", "#6baed6", "#9e9ac8", "#525252","#31a354","#a1d99b","#9ecaef" )) + 
  scale_shape_manual(values = c(19, 1)) +
  ylab("Conc (-)") + 
  theme_bw() + 
  labs(title = "ARG distribution across all data normalized by PMMoV")
```

Calculating summary stats about EVA data as a whole. 
```{r EVA_summary}
#Merge data with metadata
data_08_EVA_mer <- merge(data_06_EVA_filt, data_02_met_mod, by = c("sample_id"), all = TRUE) %>% filter(!is.na(plant.y)) %>% filter(!is.na(plant.x))

#Number of unique samples 
dim(data_08_EVA_mer)
length(unique(data_08_EVA_mer$sample_id))
length(unique(data_08_EVA_mer$plant.y))
length(unique(data_08_EVA_mer$state))
length(unique(data_08_EVA_mer$city))

#Look at which samples have data for ARG but not EVA
setdiff(unique(data_08_ARG_mer$sample_id), unique(data_08_EVA_mer$sample_id))
```

After QA/QC, we have 442 samples and 881 measurements that represent 164 plants in 40 states and 134 cities in the US. This is excluding the 2 samples from site 87 that Dora said should be excluded because they don't get exported (87-240519, 87-240521). 

The following samples were not run with 16S and rpoB due to lack of sample after running the ARGs: 54-240522 (Little River, Roswell GA but have samples from other dates), 65-240522 (Pinson, AL but have two other dates), 225-240516 (Bridgewater NJ but have two other dates). 78-240520 16S & rpoB was taken out as part of the QAQC criteria #2. 

Plotting the distribution of 16S & rpoB:

```{r EVA_dist}
ggplot(data_08_EVA_mer, aes(x = target, y = dry_gc_g, color = target)) + 
  geom_jitter(color = "darkgrey", size = 0.5, alpha = 0.9) +
  geom_boxplot(alpha = 0.7) +
  scale_y_continuous(trans = 'log10') + 
  scale_color_manual(values = c("#f768a1","#fbb4b9" )) + 
  scale_shape_manual(values = c(19, 1)) +
  ylab("Conc (cp/g dry weight)") + 
  theme_bw() + 
  labs(title = "Normalization target distribution across all data")
```

## 1.6 Outlier detection 
Big outliers in 16S and rpoB can really skew the normalized data so assuming that 16S and rpoB distribution among all sites should be fairly normally distributed, we'll see if there are any big outliers according to the Rosner's test. 

```{r EVA_normality}
#Subset data into 16S and rpoB 
data_08_EVA_16S <- filter(data_08_EVA_mer, target == "16S")
data_08_EVA_rpoB <- filter(data_08_EVA_mer, target == "rpoB2")

#Testing for normality 
shapiro.test(data_08_EVA_16S$dry_gc_g)
shapiro.test(data_08_EVA_rpoB$dry_gc_g)

#Testing for normality after log transforming
shapiro.test(log10(data_08_EVA_16S$dry_gc_g))
shapiro.test(log10(data_08_EVA_rpoB$dry_gc_g))

#Plotting histogram to see distribution
ggplot(data_08_EVA_mer, aes(x = log10(dry_gc_g))) +
  geom_histogram() +
  facet_wrap(vars(target), scales = "free_x")

#Plotting qqplots to see distribution 
qqnorm(log10(data_08_EVA_16S$dry_gc_g))
qqline(log10(data_08_EVA_16S$dry_gc_g))

qqnorm(log10(data_08_EVA_rpoB$dry_gc_g))
qqline(log10(data_08_EVA_rpoB$dry_gc_g))

```

Shapiro-wilk test shows that the raw and log-transformed data of 16S and rpoB are not statistically normally distributed but the histograms of the log transformed values seem to be visually close to normal distribution and majority of our points on the qq plot fall on the qqline so we'll assume that log-transformed concentrations follow an approximately normal distribution and use the Rosner's test for outliers. 

```{r EVA_outlier}

#Rosner's test for outliers assuming approximately normal 
rosner_16S <- rosnerTest(log10(data_08_EVA_16S$dry_gc_g), k=10)
rosner_rpoB <- rosnerTest(log10(data_08_EVA_rpoB$dry_gc_g), k=10)

print(rosner_16S)
print(rosner_rpoB)

#Taking out outliers identified by Rosner's 
data_09_EVA_16S <- data_08_EVA_16S[-c(249),]
data_09_EVA_rpoB <- data_08_EVA_rpoB[-c(22, 251, 24, 23),]

data_09_EVA <- rbind(data_09_EVA_16S, data_09_EVA_rpoB)

#Retest to see if taking out the outliers affected normality of the distribution
shapiro.test(log10(data_09_EVA_16S$dry_gc_g))
shapiro.test(log10(data_09_EVA_rpoB$dry_gc_g))

qqnorm(log10(data_09_EVA_16S$dry_gc_g))
qqline(log10(data_09_EVA_16S$dry_gc_g))

qqnorm(log10(data_09_EVA_rpoB$dry_gc_g))
qqline(log10(data_09_EVA_rpoB$dry_gc_g))
```
With Rosner's test on the log-transformed data, we identified one outlier (index = 249: 23-240517) that's the lowest point in 16S distribution and four outliers (index = 22: 107-240520, 251: 23-240517, 24: 107-240523, 23: 107-240522) that's the lowest four points in rpoB distribution. 

Taking those points out, it still does not make the distribution normal by shapiro-wilk but we'll take them out for future analysis.  

## 1.7 Normalizers
Looking at the relationship between 16S and rpoB: 

```{r EVA_ratio}
#Calculate ratio of 16S to rpoB 
data_09_EVA_SB <- merge(data_09_EVA_16S, data_09_EVA_rpoB, 
                        by = c("sample_id", "date", "plant.x", "plant.y", 
                               "city", "state", "site", "zipcode", "population", 
                               "fips"), 
                        suffixes = c("_S", "_B"),
                        all = TRUE) %>% 
  mutate(ratio = dry_gc_g_S / dry_gc_g_B)

mean(data_09_EVA_SB$ratio, na.rm = TRUE)
median(data_09_EVA_SB$ratio, na.rm= TRUE)

#Looking to see if there's a linear relationship between 16S and rpoB
ggplot(data_09_EVA_SB, aes(x = dry_gc_g_B, y = dry_gc_g_S)) + 
  geom_point(shape = 1) + 
  stat_smooth(method = "lm", formula = y~x, geom = "smooth") + 
  theme_bw() + 
  ylab("Conc of 16S (cp/g dry weight)") + 
  xlab("Conc of rpoB (cp/g dry weight)") +
  labs(title = "16S vs rpoB Concentration")

lm_EVA <- lm(dry_gc_g_S ~ dry_gc_g_B, data = data_09_EVA_SB)
summary(lm_EVA)
```

The linear regression between 16S vs rpoB shows that there is a significant relationship (p-value <2.2e-16) with R2 value of 0.78. The relationship can be described as y = 67.6x - 4.12e11 with both slope and intercept being statistically significant. 

We'll also look at the distribution normalized by 16S and rpoB. 

```{r ARG_norm}
#Merge ARG and EVA data frame and normalize by concentration of 16S and rpoB
data_10_mer <- merge(data_08_ARG_mer, data_09_EVA_SB, 
                     by = c("sample_id", "date", "plant.x", "plant.y", "city", 
                            "state", "site", "zipcode", "population", "fips"), 
                     suffixes = c("_A", "_E"),
                     all = TRUE) %>% 
  mutate(conc_norm_S = dry_gc_g/dry_gc_g_S,
         conc_norm_B = dry_gc_g/dry_gc_g_B,
         conc_norm_P = dry_gc_g/PMMoV_gc_g)

#plant.y seems to be the more specific plant name so we'll set that as the base plant name
data_10_mer <- rename(data_10_mer, plant = plant.y)

#Distribution when normalized by 16S 
ggplot(data_10_mer, aes(x = target, y = conc_norm_S, color = target)) + 
  geom_jitter(aes(shape = detect), color = "darkgrey", size = 0.5, alpha = 0.9) +
  geom_boxplot(alpha = 0.7) +
  scale_y_continuous(trans = 'log10') + 
  scale_color_manual(values = c("#05386d","#54278f", "#08519c", "#fc9272", "#756bb1", "#3182bd", "#6baed6", "#9e9ac8", "#525252","#31a354","#a1d99b","#9ecaef" )) + 
  scale_shape_manual(values = c(19, 1)) +
  ylab("Conc (-)") + 
  theme_bw() +
  labs(title = "ARG distribution normalized by 16S")

#Distribution when normalized by rpoB
ggplot(data_10_mer, aes(x = target, y = conc_norm_B, color = target)) + 
  geom_jitter(aes(shape = detect), color = "darkgrey", size = 0.5, alpha = 0.9) +
  geom_boxplot(alpha = 0.7) +
  scale_y_continuous(trans = 'log10') + 
  scale_color_manual(values = c("#05386d","#54278f", "#08519c", "#fc9272", "#756bb1", "#3182bd", "#6baed6", "#9e9ac8", "#525252","#31a354","#a1d99b","#9ecaef" )) + 
  scale_shape_manual(values = c(19, 1)) +
  ylab("Conc (-)") + 
  theme_bw() + 
  labs(title = "ARG distribution normalized by rpoB")

```
Either normalization method doesn't change the distribution noticeably, which makes sense. 

## 1.8 Secondary datasets

Importing the secondary database to be used with our ARG concentration data are:
1. Antibiotic consumption data from EPIC 
2. Dataset with various indicators such as EJI, SVI, etc 
3. Animal agriculture data that has number of chickens, cows, and hogs in the sewershed

```{r sec_import}
#Antibiotic drug consumption 
sec_01_ABD_raw <- read_csv("Secondary Data - Compile_dataframe.csv")
sec_02_ABD_filt <- sec_01_ABD_raw %>% 
  select(Plant, city, state, site_name, `Pen-Ceph1-4MRSA`, Colistin, Carbapenems, 
         `Pen/CephALL`, Tetracycline, Vancomycin, TotalEncounters, `%Pen-Ceph1-4MRSA`,
         `%Colistin`, `%Carbapenems`, `%Pen/CephALL`, `%Tetracycline`, `%Vancomycin`) %>%
  distinct() %>% 
  rename(plant = Plant, 
         site = site_name)

#Replace "10 or fewer" with 5
sec_02_ABD_filt[sec_02_ABD_filt == "10 or fewer"] <- "5"
sec_02_ABD_filt$Colistin <- as.numeric(sec_02_ABD_filt$Colistin)
sec_02_ABD_filt$Carbapenems <- as.numeric(sec_02_ABD_filt$Carbapenems)

#Various indicators 
sec_01_ind_raw <- read_csv("WWSCAN_sewershed_indicators_ARG_updatedSVI.csv") %>% 
  rename(plant = name,
         site = site_name)

#Fixing typos and differences in site names 
data_10_mer$city <- gsub("\\.", "", data_10_mer$city) #to match cities in other datasets


#Animal agriculture data 
sec_01_aa_raw <- read_csv("WWSCAN_sewershed_agriculture.csv")
```

Since we're treating different time points for each WWTP as replicates, we'll summarize the dPCR data so that each treatment plant has one value for one target and format other data frames to be wide to be merged. 
```{r data_prep, fig.width = 7, fig.height = 2}
#Treating different time points for each WWTP as replicates 
data_11_sum <- data_10_mer %>% group_by(site, plant, city, state, zipcode, 
                                        population, fips, target) %>%
  summarise(mean_ARG_raw = mean(dry_gc_g, na.rm = TRUE),
            mean_PMMoV = mean(PMMoV_gc_g, na.rm = TRUE),
            mean_16S = mean(dry_gc_g_S, na.rm = TRUE), 
            mean_rpoB = mean(dry_gc_g_B, na.rm = TRUE), 
            mean_ARG_P = mean(conc_norm_P, na.rm = TRUE), 
            mean_ARG_S = mean(conc_norm_S, na.rm = TRUE), 
            mean_ARG_B = mean(conc_norm_B, na.rm = TRUE), 
            n = n()) %>%
  mutate(norm_ARG_P = mean_ARG_raw/mean_PMMoV,
         norm_ARG_S = mean_ARG_raw/mean_16S,
         norm_ARG_B = mean_ARG_raw/mean_rpoB) %>%
  mutate(ratio_P = mean_ARG_P/norm_ARG_P,
         ratio_S = mean_ARG_S/norm_ARG_S,
         ratio_B = mean_ARG_B/norm_ARG_B)

#See if there's a big difference between average of normalized values vs normalized average values
ggplot(data_11_sum) +
#  geom_boxplot(aes(x = target, y = ratio_P)) + 
#  geom_boxplot(aes(x = target, y = ratio_S)) +
  geom_boxplot(aes(x = target, y = ratio_B)) +
  geom_hline(yintercept = 1, color = "darkgrey", linetype = "dashed") +
  scale_y_continuous(trans = 'log10', labels = function(x) format(x, scientific = TRUE))   
```

Variable mean_ARG_X is average of normalized values and norm_ARG_X is normalizing with averaged values. It seems that overall average of normalized values are usually higher than the corresponding normalizing with averaged values (above the dashed line, which is ratio of 1). Using the average of normalized values make more sense since then the normalizer and the target would be date-matched; therefore, we'll use the average of normalized values in further analysis. 

The animal agriculture data currently has various categories of cattle and chickens. For cattle, the following choices are available: cattle, excluding cows; cattle, cows; cattle, cows, beef; cattle, cows, milk; cattle, incl calves; cattle, on feed. Since cattle, incl calves is the broadest category, we'll use that data. 

For chicken, the following data is available: chickens, broilers; chickens, layers; chickens, pullets, replacement; chickens, roosters. Since all chickens are relevant to our analysis, we'll add the numbers up to get a total number of chickens. 

There is only one data item for hogs. 

```{r prep_aa}
#Filter for "cattle, incl calves" 
sec_01_aa_cow <- sec_01_aa_raw[grepl("CATTLE", sec_01_aa_raw$Data.Item, ignore.case = TRUE), ] %>% 
  filter(Data.Item == "CATTLE, INCL CALVES - INVENTORY")

#Add up all types of chicken for an inclusive number of chickens for each sewershed 
sec_01_aa_ckn <- sec_01_aa_raw[grepl("CHICKENS", sec_01_aa_raw$Data.Item, ignore.case = TRUE), ] %>%
  group_by(name) %>%
  summarise(n_cnty_data_any = sum(n_cnty_data_any, na.rm = TRUE),
            n_cnty_data_value = sum(n_cnty_data_value, na.rm = TRUE),
            Value = sum(Value, na.rm = TRUE),
            n_cnty_total = sum(n_cnty_total, na.rm = TRUE))
sec_01_aa_ckn$Data.Item <- "CHICKEN, TOTAL"

#Hogs 
sec_01_aa_pig <- sec_01_aa_raw[grepl("HOGS", sec_01_aa_raw$Data.Item, ignore.case = TRUE), ]

#Merge cow, chicken, and pig together 
sec_02_aa_filt <- rbind(sec_01_aa_cow, sec_01_aa_ckn, sec_01_aa_pig) %>% 
  rename(plant = name)

#Make into wide data frame
sec_03_aa_wide <- sec_02_aa_filt %>% 
  select(plant, Data.Item, Value) %>% 
  pivot_wider(names_from = Data.Item, values_from = Value)

```

Merge everything together to make one big data frame 
```{r merge_all}
#Merge ABD with dPCR data by state 
#data_all_1 <- merge(data_11_sum, sec_02_ABD_filt, 
#                    by = c("plant", "city", "state", "site"), 
#                    all.x = TRUE)
#data_all_2 <- merge(data_all_1, sec_02_aa_filt, by = c("plant"), all.x = TRUE)

data_all_2 <- merge(data_11_sum, sec_03_aa_wide, by = c("plant"), all.x = TRUE)
data_all_3 <- merge(data_all_2, sec_01_ind_raw, by = c("site", "plant", "city", "state"), all.x = TRUE)
```

## 1.9 Combine ARG as scores to express as ARG burden
We'll look at ARG concentrations as individual concentrations but also as overall or drug specific burden by making z-scores that can be added up. Z-scores were calculated by looking at overall distribution across all samples for that target in the dataset and comparing the concentration to the average and standard deviation of that distribution. 

```{r z-scores}

#New column for z scores
data_all_3$zscore <- NA

#Assign z scores 
for (i in 1:length(data_all_3$plant)) {
  targ <- data_all_3$target[i]
  average = mean(subset(data_all_3, target == targ)$mean_ARG_S, na.rm = TRUE)
  std = sd(subset(data_all_3, target == targ)$mean_ARG_S, na.rm = TRUE)
  data_all_3$zscore[i] <- (data_all_3$mean_ARG_S[i] - average)/std
}

#Assigning overall ARG burden 
data_burden_tot <- data_all_3 %>% 
  group_by(site, plant, city, state) %>%
  summarise(Burden_all = sum(zscore)) 

#Combining class of antibiotics 
#Betalactamase: CMY, CTX-M, KPC, mecA, NDM, OXA-48, TEM, VIM
data_burden_bla <- data_all_3 %>% 
  subset(target == "CMY" | target == "CTX-M" | 
         target == "KPC" | target == "MecA" | target == "NDM" |
         target == "OXA-48" | target == "TEM" | target == "VIM") %>%
  group_by(site, plant, city, state) %>%
  summarise(Burden_bla = sum(zscore)) 

#Colistin: mcr-1
data_burden_col <- data_all_3 %>% 
  subset(target == "MCR-1") %>%
  group_by(site, plant, city, state) %>%
  summarise(Burden_col = sum(zscore)) 

#Vancomycin: vanA, vanB
data_burden_van <- data_all_3 %>% 
  subset(target == "VanA" | target == "VanB") %>%
  group_by(site, plant, city, state) %>%
  summarise(Burden_van = sum(zscore)) 

#Tetracycline: tetW 
data_burden_tet <- data_all_3 %>% 
  subset(target == "TetW") %>%
  group_by(site, plant, city, state) %>%
  summarise(Burden_tet = sum(zscore)) 

#Putting everything together
data_burden <- merge(data_burden_tot, data_burden_bla, 
                     by = c("site", "plant", "city", "state"),
                     all = TRUE) %>%
  merge(data_burden_col,
        by = c("site", "plant", "city", "state"),
        all = TRUE) %>%
  merge(data_burden_van, 
        by = c("site", "plant", "city", "state"),
        all = TRUE) %>% 
  merge(data_burden_tet,
        by = c("site", "plant", "city", "state"),
        all = TRUE)

#Re-merge with secondary data
data_burden_mer <- merge(data_burden, sec_03_aa_wide, by = c("plant"), all.x = TRUE) %>%
  merge(sec_01_ind_raw, by = c("site", "plant", "city", "state"), all.x = TRUE)
```

These are the final data sets that we're working with so we'll export them. 

```{r data_export}
#Export data frame 
write.csv(data_10_mer, "dPCR_data_final.csv", row.names = FALSE)
write.csv(data_11_sum, "dPCR_data_averaged.csv", row.names = FALSE)
write.csv(data_all_3, "data_all.csv", row.names = FALSE)
write.csv(data_burden_mer, "data_ARGburden.csv", row.names = FALSE)
```
